{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#  Import and read the charity_data.csv.\n",
    "import pandas as pd \n",
    "application_df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
    "application_df.drop(['EIN', 'NAME'], axis=1, inplace=True)\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "unique_values = application_df.nunique()\n",
    "\n",
    "# Display the number of unique values in each column\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at APPLICATION_TYPE value counts for binning\n",
    "application_type_values = application_df[\"APPLICATION_TYPE\"].value_counts()\n",
    "\n",
    "# Display the number of unique values in each column\n",
    "print(application_type_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose a cutoff value and create a list of application types to be replaced\n",
    "# use the variable name `application_types_to_replace`\n",
    "application_types_to_replace = list(application_type_values[application_type_values<500].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for app in application_types_to_replace:\n",
    "    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "\n",
    "# Check to make sure binning was successful\n",
    "application_df['APPLICATION_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at CLASSIFICATION value counts for binning\n",
    "# Display value counts for CLASSIFICATION column\n",
    "classification_counts = application_df['CLASSIFICATION'].value_counts()\n",
    "print(classification_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You may find it helpful to look at CLASSIFICATION value counts >1\n",
    "# Filter and display value counts for CLASSIFICATION column greater than 1\n",
    "classification_counts_greater_than_1 = application_df['CLASSIFICATION'].value_counts()[application_df['CLASSIFICATION'].value_counts() > 1]\n",
    "print(classification_counts_greater_than_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose a cutoff value and create a list of classifications to be replaced\n",
    "# use the variable name `classifications_to_replace`\n",
    "classifications_to_replace = list(classification_counts[classification_counts<1000].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for cls in classifications_to_replace:\n",
    "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "    \n",
    "# Check to make sure binning was successful\n",
    "application_df['CLASSIFICATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "# Select categorical columns for conversion\n",
    "categorical_cols = ['APPLICATION_TYPE', 'AFFILIATION', 'CLASSIFICATION', 'USE_CASE', 'ORGANIZATION', 'INCOME_AMT', 'SPECIAL_CONSIDERATIONS'] \n",
    "\n",
    "# Create dummy variables for categorical columns\n",
    "application_df = pd.get_dummies(application_df, columns=categorical_cols)\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "\n",
    "# Features (all columns except 'IS_SUCCESSFUL' which is the target)\n",
    "features = application_df.drop('IS_SUCCESSFUL', axis=1).values\n",
    "\n",
    "# Target\n",
    "target = application_df['IS_SUCCESSFUL'].values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the SVM model\n",
    "svm_model = SVC(kernel='rbf')  # 'rbf' kernel is a common choice; you can try others too\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV 1/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.690 total time=  36.8s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.1, 1, 10, 100],\n",
    "              'gamma': [1, 0.1, 0.01, 0.001],\n",
    "              'kernel': ['rbf']}  \n",
    "\n",
    "# Instantiate the model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Use GridSearchCV to find the best parameters\n",
    "grid = GridSearchCV(svm_model, param_grid, refit=True, verbose=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and predict with the optimized model\n",
    "best_params = grid.best_params_\n",
    "optimized_model = grid.best_estimator_\n",
    "predictions = optimized_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"Model Accuracy after optimization: {accuracy}\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualize hyperparameter performance\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results = results.pivot_table(index='param_C', columns='param_gamma', values='mean_test_score')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(results, annot=True, cmap='viridis')\n",
    "plt.title('Grid Search Mean Test Scores')\n",
    "plt.show()\n",
    "\n",
    "# Narrow down the parameter range\n",
    "param_grid_narrow = {'C': [5, 10, 15],\n",
    "                     'gamma': [1, 0.1, 0.01, 0.001],\n",
    "                     'kernel': ['rbf']}\n",
    "\n",
    "# Re-run GridSearchCV with the narrowed parameter range\n",
    "grid_narrow = GridSearchCV(svm_model, param_grid_narrow, refit=True, verbose=3)\n",
    "grid_narrow.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and predict with the optimized model\n",
    "best_params_narrow = grid_narrow.best_params_\n",
    "optimized_model_narrow = grid_narrow.best_estimator_\n",
    "predictions_narrow = optimized_model_narrow.predict(X_test)\n",
    "accuracy_narrow = accuracy_score(y_test, predictions_narrow)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Model Accuracy after further optimization: {accuracy_narrow}\")\n",
    "print(f\"Best Parameters after further optimization: {best_params_narrow}\")\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "conf_matrix = confusion_matrix(y_test, predictions_narrow)\n",
    "classification_rep = classification_report(y_test, predictions_narrow)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = accuracy.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "svm_model.save(\"AlphabetSoupCharity_Optimized.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
